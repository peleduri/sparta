name: Aggregate Scan Results

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      reports-dir:
        description: 'Directory containing vulnerability reports'
        required: false
        type: string
        default: 'vulnerability-reports'
      output-dir:
        description: 'Output directory for aggregated data'
        required: false
        type: string
        default: 'aggregated'
  workflow_call:
    inputs:
      reports-dir:
        description: 'Directory containing vulnerability reports'
        required: false
        type: string
        default: 'vulnerability-reports'
      output-dir:
        description: 'Output directory for aggregated data'
        required: false
        type: string
        default: 'aggregated'

jobs:
  aggregate:
    name: Aggregate Scan Results
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Aggregate scan results
        env:
          REPORTS_DIR: ${{ inputs.reports-dir || github.event.inputs.reports-dir || 'vulnerability-reports' }}
          OUTPUT_DIR: ${{ inputs.output-dir || github.event.inputs.output-dir || 'aggregated' }}
        run: |
          python3 << 'EOF'
          import os
          import json
          import sys
          from pathlib import Path
          from collections import defaultdict
          from datetime import datetime

          reports_dir = Path(os.environ['REPORTS_DIR'])
          output_dir = Path(os.environ['OUTPUT_DIR'])

          def load_scan_reports(reports_dir):
              reports = []
              if not reports_dir.exists():
                  print(f"Error: Reports directory '{reports_dir}' does not exist")
                  return reports
              
              for report_file in reports_dir.rglob('trivy-report.json'):
                  try:
                      with open(report_file, 'r') as f:
                          report_data = json.load(f)
                      
                      parts = report_file.parts
                      if len(parts) >= 4:
                          org = parts[-4] if len(parts) >= 4 else 'unknown'
                          repo = parts[-3] if len(parts) >= 3 else 'unknown'
                          scan_date = parts[-2] if len(parts) >= 2 else 'unknown'
                      else:
                          org = 'unknown'
                          repo = 'unknown'
                          scan_date = 'unknown'
                      
                      reports.append({
                          'file': str(report_file),
                          'org': org,
                          'repo': repo,
                          'scan_date': scan_date,
                          'data': report_data
                      })
                  except Exception as e:
                      print(f"Warning: Error processing {report_file}: {e}", file=sys.stderr)
                      continue
              
              return reports

          def aggregate_statistics(reports):
              stats = {
                  'total_scans': 0,
                  'total_repositories': set(),
                  'total_orgs': set(),
                  'scan_dates': set(),
                  'cve_index': defaultdict(list),
                  'repo_vulnerabilities': defaultdict(lambda: {
                      'total': 0,
                      'critical': 0,
                      'high': 0,
                      'medium': 0,
                      'low': 0,
                      'cves': set()
                  }),
                  'severity_distribution': defaultdict(int),
                  'package_vulnerabilities': defaultdict(lambda: {
                      'cves': set(),
                      'repos': set()
                  })
              }
              
              for report in reports:
                  if 'error' in report['data']:
                      continue
                  
                  stats['total_scans'] += 1
                  stats['total_repositories'].add(f"{report['org']}/{report['repo']}")
                  stats['total_orgs'].add(report['org'])
                  stats['scan_dates'].add(report['scan_date'])
                  
                  repo_key = f"{report['org']}/{report['repo']}"
                  
                  if 'Results' in report['data']:
                      for result in report['data'].get('Results', []):
                          if 'Vulnerabilities' in result:
                              for vuln in result['Vulnerabilities']:
                                  cve_id = vuln.get('VulnerabilityID', '')
                                  severity = vuln.get('Severity', 'UNKNOWN').upper()
                                  pkg_name = vuln.get('PkgName', 'unknown')
                                  
                                  if cve_id:
                                      stats['cve_index'][cve_id].append({
                                          'repository': repo_key,
                                          'org': report['org'],
                                          'repo': report['repo'],
                                          'scan_date': report['scan_date'],
                                          'severity': severity,
                                          'package': pkg_name,
                                          'package_version': vuln.get('InstalledVersion', ''),
                                          'fixed_version': vuln.get('FixedVersion', ''),
                                      })
                                      
                                      stats['repo_vulnerabilities'][repo_key]['total'] += 1
                                      stats['repo_vulnerabilities'][repo_key]['cves'].add(cve_id)
                                      if severity == 'CRITICAL':
                                          stats['repo_vulnerabilities'][repo_key]['critical'] += 1
                                      elif severity == 'HIGH':
                                          stats['repo_vulnerabilities'][repo_key]['high'] += 1
                                      elif severity == 'MEDIUM':
                                          stats['repo_vulnerabilities'][repo_key]['medium'] += 1
                                      elif severity == 'LOW':
                                          stats['repo_vulnerabilities'][repo_key]['low'] += 1
                                      
                                      stats['severity_distribution'][severity] += 1
                                      
                                      stats['package_vulnerabilities'][pkg_name]['cves'].add(cve_id)
                                      stats['package_vulnerabilities'][pkg_name]['repos'].add(repo_key)
              
              stats['total_repositories'] = sorted(list(stats['total_repositories']))
              stats['total_orgs'] = sorted(list(stats['total_orgs']))
              stats['scan_dates'] = sorted(list(stats['scan_dates']))
              
              for repo_key in stats['repo_vulnerabilities']:
                  stats['repo_vulnerabilities'][repo_key]['cves'] = sorted(list(stats['repo_vulnerabilities'][repo_key]['cves']))
              
              for pkg_name in stats['package_vulnerabilities']:
                  stats['package_vulnerabilities'][pkg_name]['cves'] = sorted(list(stats['package_vulnerabilities'][pkg_name]['cves']))
                  stats['package_vulnerabilities'][pkg_name]['repos'] = sorted(list(stats['package_vulnerabilities'][pkg_name]['repos']))
              
              return stats

          def generate_summary_report(stats):
              lines = []
              lines.append("=" * 80)
              lines.append("Vulnerability Scan Aggregation Summary")
              lines.append("=" * 80)
              lines.append(f"\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
              lines.append("\nOverall Statistics:")
              lines.append(f"  Total Scans: {stats['total_scans']}")
              lines.append(f"  Total Repositories: {len(stats['total_repositories'])}")
              lines.append(f"  Total Organizations: {len(stats['total_orgs'])}")
              if stats['scan_dates']:
                  lines.append(f"  Scan Date Range: {min(stats['scan_dates'])} to {max(stats['scan_dates'])}")
              lines.append(f"  Unique CVEs Found: {len(stats['cve_index'])}")
              
              lines.append("\nSeverity Distribution:")
              for severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'UNKNOWN']:
                  count = stats['severity_distribution'].get(severity, 0)
                  if count > 0:
                      lines.append(f"  {severity}: {count}")
              
              lines.append("\nTop 10 Repositories by Vulnerability Count:")
              sorted_repos = sorted(
                  stats['repo_vulnerabilities'].items(),
                  key=lambda x: x[1]['total'],
                  reverse=True
              )[:10]
              
              for repo, repo_stats in sorted_repos:
                  lines.append(f"  {repo}: {repo_stats['total']} vulnerabilities "
                              f"(C: {repo_stats['critical']}, H: {repo_stats['high']}, "
                              f"M: {repo_stats['medium']}, L: {repo_stats['low']})")
              
              lines.append("\nTop 10 Most Common CVEs:")
              sorted_cves = sorted(
                  stats['cve_index'].items(),
                  key=lambda x: len(x[1]),
                  reverse=True
              )[:10]
              
              for cve_id, occurrences in sorted_cves:
                  lines.append(f"  {cve_id}: Found in {len(occurrences)} repository scan(s)")
              
              lines.append("\n" + "=" * 80)
              
              return "\n".join(lines)

          print("Loading scan reports...")
          reports = load_scan_reports(reports_dir)
          
          if not reports:
              print("No reports found.")
              sys.exit(1)
          
          print(f"Loaded {len(reports)} scan reports")
          print("Aggregating statistics...")
          
          stats = aggregate_statistics(reports)
          
          output_dir.mkdir(parents=True, exist_ok=True)
          
          stats_file = output_dir / 'statistics.json'
          with open(stats_file, 'w') as f:
              json.dump(stats, f, indent=2)
          print(f"Saved statistics to {stats_file}")
          
          cve_index_file = output_dir / 'cve-index.json'
          with open(cve_index_file, 'w') as f:
              json.dump(dict(stats['cve_index']), f, indent=2)
          print(f"Saved CVE index to {cve_index_file}")
          
          repo_summary_file = output_dir / 'repository-summary.json'
          with open(repo_summary_file, 'w') as f:
              json.dump(dict(stats['repo_vulnerabilities']), f, indent=2)
          print(f"Saved repository summary to {repo_summary_file}")
          
          summary_report = generate_summary_report(stats)
          summary_file = output_dir / 'summary.txt'
          with open(summary_file, 'w') as f:
              f.write(summary_report)
          print(f"Saved summary report to {summary_file}")
          
          print("\n" + summary_report)
          EOF

      - name: Upload aggregated results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: aggregated-scan-results
          path: ${{ inputs.output-dir }}
          retention-days: 90

      - name: Set workflow summary
        if: always()
        run: |
          echo "## Aggregation Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f ${{ inputs.output-dir }}/summary.txt ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat ${{ inputs.output-dir }}/summary.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

